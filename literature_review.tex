\section{Literature Review}

Due to the novelty of conversational RAG, there are only a few papers, all published in the last two years, covering the topic. Accordingly, this literature review will cover six exemplary papers, surveying the models and techniques they employ and the results they have reported. Then, in the next section, the information will be synthesized to create a general taxonomy for conversational RAG models.


\subsection{Memory Matters: The Need to Improve Long-Term Memory in LLM Agents (Hatalis et al., 2024)}

\cite{Hatalis2024} is a survey of memory management approaches in LLMs, with a focus on long-term memory. They propose that conversational agents could have long-term memories modelled after the common model of cognition, which divides human memory into four subdivisions: procedural memory, which contains current goals, plans, skills; semantic memory, which records facts about the world and user; episodic memory, which focuses on events that have occurred; and working memory, which temporarily holds the most recent memories. According to them, all types of long-term memory can be implemented as a single cohesive unit in a LLM and the context window can act as the working memory.

They recognize that because of the fixed-size of context windows in LLMs, long-term memory solutions have become necessary. According to them, currently, most proposed and implemented solutions make use of knowledge bases, specifically vector databases. From these, memories are usually retrieved using similarity matching, in which the top-\textit{k} memories are returned according to cosine similarity, euclidean distance, or dot product.

They find, however, that the use of vector databases have limitations. First is that different types of long-term memories, as described by the common model of cognition, could interfere with one another if they are not separated. For example, the semantic memory that the earth is round is treated the same as an episodic memory of someone claiming that the earth is flat. For a model that does not distinguish such memories, the earth could be erroneously remembered to be flat, if the similarity score is higher for the latter memory. Second, performance issues can occur as the long-term memory grows and the vector databases get larger. The authors propose a forgetting mechanism as a solution, to keep the long-term memory within a manageable size.


\subsection{KG-RAG: Bridging the Gap Between Knowledge and Creativity (Sanmartín, 2024)}

In contrast to the models mentioned in the previous paper, \cite{Sanmartin2024} presents a LLM that utilizes a knowledge graph, instead of a knowledge base, to store information.

Knowledge graphs are an alternative way of storing information using structured representations of entities and the relationships between them. This typically takes the form of triples, containing a pair of nodes, usually representing entities, and their relationship: for example, "The dog likes bones" would be rendered as the triple
	
\begin{displayquote}
(\textit{n1}: dog, \textit{r}: like, \textit{n2}: bone)
\end{displayquote}
	
\noindent where the nodes \textit{n1} ("dog") and \textit{n2} ("bone") are linked by the relation \textit{r1} ("like"). Then, as new information is processed, additional nodes may be attached to the previous two: given the sentence "I have a dog", the node \textit{n3} ("I") would be linked to \textit{n2} by the relation \textit{r2} ("have").

Sanmartín's model, KG-RAG, constructs a knowledge graph from unstructured text and then retrieves relevant info for use in generation, in hopes of mitigating hallucinations, issues with processing long contexts, and catastrophic forgetting (i.e. new information causes model to forget old information), a prospect that has been recently explored by other researchers (\cite{Guan2023}, \cite{Yang2024}). He argues that knowledge graphs have an advantage over knowledge bases because information retrieval by vector similarity, which is what most knowledge bases use, is not granular enough: it returns too much information, of which not all might be relevant.
	
Knowledge graphs, in contrast, allow for more efficient information retrieval methods, since the information therein is much more structured and efficient, allowing information to be retrieved based on the entities involved, rather than mere sentence similarity. This difference is neatly summarized by Singhal Amit, in introducing Google's Knowledge Graph: knowledge graphs look for "things, not strings" (\cite{Singhal2012}). Another advantage is that knowledge graphs evolve with new knowledge, whereas knowledge bases retain the old, incorrect knowledge which might be retrieved if found to be more similar to the query: if a white dog was painted blue between conversations, for example, and the question is the dog's color, a knowledge base might retrieve a memory from before the dog was painted and answer white; a knowledge graph, on the other hand, would have updated the color to blue, making it impossible to answer otherwise.
	
Sanmartín tests this KG-RAG model against a knowledge base and human baselines using the ComplexWebQuestions dataset, a collection of nearly 35,000 questions, recording exact matches and the corresponding F1 score. The results show that KG-RAG is worse than knowledge base RAG in terms of F1 and accuracy; however, in terms of mitigating hallucinations, the paper's primary area of interest, KG-RAG does indeed outperform the knowledge base baseline.
	
These results show that knowledge graphs could be a viable substitute for knowledge bases, but carry their own drawbacks in terms of answer accuracy. However, it must be noted that this line of research has only focused on knowledge graph RAG in the context of question-answering tasks, so their performance in conversational contexts, which require a wider set of skills outside of question-answering, remains to be seen.


\subsection{Evaluating Very Long-Term Conversational Memory of LLM Agents (Maharana et al., 2024)}

In this paper, \cite{Maharana2024} state that despite many advancements in LLMs for extended contexts and with RAG, none have truly been tested over extremely long contexts, and indeed, no sufficiently long conversational datasets currently exist. They correct this oversight by creating \textsc{LoCoMo}, a dataset of 10 very long-term dialogues, each consisting of 300 turns and 9K tokens on average and representing an average of 19 separate conversational sessions, which they use to test the memory capabilities of conversational models.

To create the \textsc{LoCoMo} dataset, and for the experiments run upon it, the researchers used LLM-based conversational models equipped with character personas (commonly called "generative agents") to generate  the conversations. Each model was equipped with: a knowledge base for a long-term memory, from which memories were retrieved by semantic similarity; a persona; and a reflection mechanism. Unlike other models in the conversational RAG pipeline, in addition to the long-term memory and the conversational history (which is typically considered the short-term memory), the authors include an additional short-term memory consisting of the summary of most recent conversational session. 
	
The memory units stored in the long-term memory require special attention. For generating the dataset, the researchers store observations generated from by a separate instance of the model which has been given the current conversational history. After each new conversational session the model extracts relevant facts about the persona and the user, and stores them in the long-term memory. With these generative agents, the authors generate the conversations that comprise the dataset, which they then refine using human annotators to correct any long-term inconsistencies to produce the final product, \textsc{LoCoMo}. 
	
Using \textsc{LoCoMo}, the authors test generative agents on three types of tasks: question-answering, graph summarization, and multi-modal dialog generation. 
	
The question-answering tasks consist of a battery of questions falling into five reasoning types: 1) single-hop (answered by remembering information from a single session); 2) multi-hop (answered by remembering information from multiple sessions); 3) temporal reasoning (answered by understanding time-related cues); 4) commonsense or world knowledge (answered by understanding basic facts about the world); and 5) adversarial (answered by understanding the user is presenting the model with a misleading or unanswerable question). For these tasks, the generative agent model was tested against a baseline LLM and an LLM with a long context window (16K tokens), as well as human participants. Furthermore, three versions of the generative agents model were tested, each differing in the type of unit stored in the long-term memory: observations, individual turns, and summaries of past conversational sessions. 
	
The graph summarization task measures causal and temporal relations by having models extract graphs based on text, and comparing them to an ideal graph. Finally, the multi-modal dialog generation task investigates if models can generate narrative-consistent responses based on past context. The results of these two tasks, however, are not relevant for the current thesis proposal, and will be ignored.
	
The results for the question-answering task found that, on average, all generative agent models outperformed the baseline LLM, and the ones storing observations and turns outperformed the long-context model. The generative agents were found to be much better at answering temporal and adversarial questions than long-context models. However, all models performed poorly in contrast to human participants. Also, observations were found to be the best overall performing type of memory unit, followed by turns and then summaries. Finally, all models performed better on single-hop questions than multi-hop ones, while models storing summaries actually performed better with answering adversarial questions than any other type of question, and even outperformed turn-storing models at that task.
	

\subsection{Generative Agents: Interactive Simulacra of Human Behavior (Park et al., 2023)}

The modern concept of "generative agents", conversational RAG LLMs, comes from \cite{Park2023}. In this bombshell paper, the authors assert that in order for LLMs to be able to handle long-term coherence and interaction, they require three abilities: 1) to remember (retrieve) relevant events, 2) to reflect on these memories given the situation, and 3) to plan and react in the moment, and in the long term, based on these reflections and the current situation. The resulting models, generative agents, are then evaluated with interviews, ablation studies, and most famously, placed in a virtual town environment and left to interact with one another with minimal outside interference.

The generative agent model is based upon the following architecture. First is a memory stream, the long-term memory, which records experiences (both conversational turns and actions made within the virtual environment), which are then retrieved based upon relevance, recency, and importance. Next comes a reflection stage, in which agents reflect upon their memories and then ask themselves questions, the answers to which they add to their memory. Finally, unlike other models in this literature review, these generative agents have a planning stage, where given conclusions from reflection and the current environments, agents form plans and enact them. These resulting actions then enter into memory, and the process begins again.
	
For evaluation, the authors interviewed agents individually with a slew of questions covering several abilities: self-knowledge, memory retrieval, plan generation, reacting to situations ("Your house is on fire!"), and reflecting on information. These were analyzed by 100 crowdsourced human participants, and ranked according to their "believability", i.e. their subjective ability to accurately mimic realistic human behavior. While model performance was found to be believable for most of these questions, the authors noted three behaviors which caused the models to fall short of human simulacrum: agents would fail to retrieve relevant memories, fabricate embellishments to their memories, and inherit overly formal speech or behavior from the language model (a hallmark of LLMs familiar to anyone who has roleplayed with one). These were done in combination with an ablation study, where the retrieval, reflection, and planning stages alternately removed before the interview. With this, the authors found that all three stages of the pipeline had almost equal effects on increasing the model's believability.

	
\subsection{MemoryBank: Enhancing Large Language Models with Long-Term Memory (Zhong et al., 2023)}
		
As previously mentioned, \cite{Maharana2024} individually compared observations, turns, and session summaries as units of memory to be stored in the long-term memory. \cite{Zhong2023} introduce a model, MemoryBank, which utilizes multiple types of memory unit at once. Memorybank stores the turns of a conversation in one knowledge base, and from these generates event summaries, i.e. summaries of the events which occur within a dialogue, which it stores in a separate knowledge base. Both sets of memory are then subjected to a version of the Ebbinghaus Forgetting Curve: memories are given a memory strength score \textit{S} which exponentially decreases as time progresses, and is reinforced every time the memory is recalled. In addition to this long-term memory system, the model maintains a personality module, a separate memory in which stores summaries of the user's personality traits and emotions, distilled from observations the model is asked to make given the conversation dialogue. 
	
According to a qualitative analysis, MemoryBank, when incorporated into a conversational model, was able to identify topics it had discussed before, and those it had not. Furthermore, the model was shown to make suggestions according to a user's aforementioned personality traits, which was taken to demonstrate the success of MemoryBank's personality module.


\subsection{Hello Again! LLM-powered Personalized Agent for Long-term Dialogue (Li et al., 2024)}

Finally, \cite{Li2024} present LD-Agent, which like \cite{Zhong2023} describes a conversational RAG model that combines a knowledge base storing dialogue event summaries with a separate module that stores persona information. Their proposed model, LD-Agent, has three principal components: an event memory perception module, a persona extraction module, and a response generation module. 
	
The event memory perception module consists of the long-term memory and short-term memory, the former storing an impersonal (not influenced by model persona) summary of the events of the previous conversational session, and the latter consisting of the dialogue for the current session, stored in a cache that gets transferred to the long-term memory after a sufficient delay in the conversation. Of particular interest is the method by which stored session summaries are retrieved: unlike any other knowledge base model presented, and like knowledge graph models, LD-Agent retrieves long-term memories by incorporating topic overlap. Alongside more the standard measures of semantic relevance and temporal recency, the model assigns scores to stored summaries based on the average proportion of common nouns in the current user turn and the summary.

In the persona extraction module, personality traits for both the user and persona are extracted from the dialogue at each turn and stored in respective persona banks. This is done in the hopes of maintaining persona consistency in the responses of the model for both participants. The final component, the response generation module, simply generates responses based on the current user turn, the relevant long-term memories, the short-term memory, and the user and agent personas.

Models incorporating LD-Agent were tested against baseline models lacking it, as well as against HAHT, a previous state-of-the-art long-term dialogue model, on two multi-session datasets of around 5 sessions each, each session consisting of around 50 turns. Each model was tested using BLEU-N, ROUGE-L, and METEOR to measure response quality, and also evaluated by human participants for coherence, fluency, and engagingness. LD-Agent was shown to outperform all baselines in long-term dialogue tasks, and ablation studies found that all three modules positively influenced performance. Of these, event memory had the biggest effect, offering more stable performance as the number of sessions increased, while the other modules suffered. Human evaluation found that topic based retrieval significantly outperforms direct semantic retrieval in accuracy and recall, and the inclusion of LD-Agent led to more coherent, fluent, and engaging dialogues.

